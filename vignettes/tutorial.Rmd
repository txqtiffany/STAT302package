---
title: "Project 3: STAT302package Tutorial"
author: "Tiffany Tian"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{STAT302package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The goal of STAT302package is to demonstrate my learning on how to develop a well-documented, well- tested, and well-explained R package from the course of STAT 302. 

## Installation

You can install the STAT302package using the following line:

``` {r install, eval = FALSE}
devtools::install_github("txqtiffany/STAT302package", build_vignette = TRUE, build_opts = c())
```

## A tutorial for `my_t.test`

This is a basic example which shows you how to use `my_t.test`:

```{r my_t.test}
library(STAT302package)
t.test_1 <- my_t.test(my_gapminder$lifeExp, "two.sided", 60)
t.test_1$p_val
t.test_2 <- my_t.test(my_gapminder$lifeExp, "less", 60)
t.test_2$p_val
t.test_3 <- my_t.test(my_gapminder$lifeExp, "greater", 60)
t.test_3$p_val
```
With the p-value cut-off of α = 0.05, we can conclude that for the first test, since `r t.test_1$p_val` > 0.05, we cannot reject the null hypothesis and there is insufficient evidence to support that the true mean of life expectency is not equal to 60. 
For the second test, since `r t.test_2$p_val` < 0.05, we can reject the null hypothesis and there is sufficient evidence to support that the true mean of life expectency is less than 60. 
For the third test, since `r t.test_3$p_val` > 0.05, we cannot reject the null hypothesis and there is insufficient evidence to support that the true mean of life expectency is greater than 60. 
Therefore, at the 5% significance level, there is sufficient evidence to support the claim that the true mean of life expectency is less than 60.


## A tutorial for `my_lm`

This is a basic example which shows you how to use `my_lm`:

```{r my_lm part 1}
library(STAT302package)
model <- my_lm(formula = lifeExp ~ gdpPercap + continent, data = my_gapminder)
print(round(model, 5))
```
The gdpPercap coefficient is really small, close to `r model[2, 1]`. Therefore, it means that the gdpPercap has a really little impact on the model for fitting lifeExp, indicating that we need to test its significance using hypothesis test. In other words, we are testing to see if the true mean of gdpPercap coefficient is 0. The null hypothesis is that it equals to 0, and the alternative hypothsis is that it doesn't equal to 0.

```{r my_lm part 2}
model[2, 4] < 0.05
```

Within the model, we can tell by the fourth column, which is our p values, that `r model[2, 4]` is less our cutoff of α = 0.05. Therefore, we reject the null coefficient that the mean of gdpPercap is 0, indicating that it is indeed significance. The reason behind why the number is so small might be because of the large scale of gdpPercap comparing to lifeExp.

Now, we will use ggplot2 to plot the Actual vs. Fitted values.

```{r my_lm part 3}
# FINDING fitted yhat
# getting the matrix of the estimates value 
my_estimates <- as.matrix(model[,"Estimate"])

# fitting data into matrix to create x matrix
x_mat <- model.matrix(lifeExp ~ gdpPercap + continent,
                      STAT302package::my_gapminder)

# matrix multiplication to get yhat 
yhat <- x_mat %*% my_estimates

my_df <- data.frame(actual = my_gapminder$lifeExp, fitted = yhat)
ggplot2::ggplot(my_df, ggplot2::aes(x = fitted, y = actual)) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(slope = 1, intercept = 0, col = "red", lty = 2) +
  ggplot2::theme_bw(base_size = 15) +
  ggplot2::labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5))
```
From the graph, we can see that our model is not a very good fit from the actual values with quite a high variance that deviates from the line. This shows that the correlation between lifeExp and the gdpPercap, continent factors are not strong enough, and that there are other factors influencing life Expectency.

## A tutorial for `my_knn_cv`

This is a basic example which shows you how to use `my_knn_cv`:

```{r my_knn_cv}
library(STAT302package)
library(kableExtra)
penguins_df <- na.omit(my_penguins)
train <- lapply(penguins_df[c("bill_length_mm", "bill_depth_mm",
                              "flipper_length_mm", "body_mass_g")], as.numeric)
cl <- as.numeric(penguins_df$species)
train_err <- c()
cv_err <- c()
for (i in 1:10) {
  knn_model <- my_knn_cv(train, cl, i, 5)
  cv_err[i] <- knn_model$cv_err
  train_err[i] <- sum(knn_model$class != cl) / length(my_penguins$species)
}
err_table <- data.frame("k_nn" = c(1:10), "train_err" = train_err, "cv_err" = cv_err)
kable_styling(kable(err_table))
```

State which model you would choose based on the training misclassification rates and which model you would choose based on the CV misclassification rates.

Base on the training misclassification, besides the trivial set of k_nn = 1 where the error trivially equals to 0, I would choose k_nn = 2 with the lowest error. Base on the CV misclassification rates, I would choose k_nn = 1 with the lowest error.

However, in practice, I would choose k_nn = 7 because that is an optimal point where both the training and misclassification errors dropped as K increase, and that the number of neighbors isn't too small for it to be overfitting. We're using cross-validation here which is a technique that is used to protect against overfitting in a predictive model. We do this by making a fixed number of folds (in this case, 5) of the data, run the analysis on each fold, and then average the overall error estimate.


## A tutorial for `my_rf_cv`

This is a basic example which shows you how to use `my_rf_cv`:

```{r my_rf_cv}
library(STAT302package)
```
